/**
 * Exemplo 5: Integra√ß√£o com LLM
 * 
 * Este exemplo demonstra:
 * 1. Como criar spans para intera√ß√µes com LLMs
 * 2. Rastrear prompts e respostas
 * 3. Manter trilha audit√°vel de conversas AI
 */

import { createSpan, signSpan, verifySpan } from '../../index'
import * as dotenv from 'dotenv'

dotenv.config()

const domain = 'llm-integration'
const privateKey = process.env.PRIVATE_KEY
const publicKey = process.env.PUBLIC_KEY

if (!privateKey || !publicKey) {
  console.error('‚ùå Chaves n√£o configuradas!')
  process.exit(1)
}

// Simular chamada para LLM (em produ√ß√£o, usar OpenAI, Anthropic, etc.)
async function callLLM(prompt: string): Promise<string> {
  // Simular lat√™ncia
  await new Promise(resolve => setTimeout(resolve, 100))
  
  // Resposta simulada
  return `Resposta gerada para: "${prompt.substring(0, 50)}..."`
}

/**
 * Wrapper que cria span para cada intera√ß√£o com LLM
 */
async function llmWithSpan(
  prompt: string,
  metadata: { userId: string; sessionId: string; traceId: string }
) {
  const startTime = Date.now()
  
  try {
    // 1. Criar span para o PROMPT
    const promptSpan = createSpan({
      type: 'llm.prompt',
      body: {
        prompt,
        model: 'gpt-4',
        maxTokens: 1000,
        temperature: 0.7
      },
      meta: {
        ...metadata,
        timestamp: startTime,
        direction: 'request'
      }
    })
    
    const signedPrompt = await signSpan(promptSpan, { domain, privateKey })
    console.log(`üì§ Prompt enviado (span: ${signedPrompt.id})`)
    
    // 2. Chamar LLM
    const response = await callLLM(prompt)
    const endTime = Date.now()
    const latency = endTime - startTime
    
    // 3. Criar span para a RESPOSTA
    const responseSpan = createSpan({
      type: 'llm.response',
      body: {
        response,
        tokensUsed: 150,
        finishReason: 'stop'
      },
      meta: {
        ...metadata,
        timestamp: endTime,
        direction: 'response',
        latencyMs: latency,
        parentId: signedPrompt.id  // Vincula ao prompt
      }
    })
    
    const signedResponse = await signSpan(responseSpan, { domain, privateKey })
    console.log(`üì• Resposta recebida (span: ${signedResponse.id})`)
    console.log(`   Lat√™ncia: ${latency}ms`)
    
    return {
      response,
      spans: {
        prompt: signedPrompt,
        response: signedResponse
      }
    }
    
  } catch (err) {
    // 4. Criar span para ERRO
    const errorSpan = createSpan({
      type: 'llm.error',
      body: {
        error: err.message,
        code: err.code || 'UNKNOWN'
      },
      meta: {
        ...metadata,
        timestamp: Date.now()
      }
    })
    
    const signedError = await signSpan(errorSpan, { domain, privateKey })
    console.error(`‚ùå Erro (span: ${signedError.id})`)
    
    throw err
  }
}

async function main() {
  console.log('üöÄ Exemplo 5: Integra√ß√£o com LLM\n')
  
  const userId = 'user_123'
  const sessionId = 'session_' + Date.now()
  const traceId = 'trace_' + Date.now()
  
  console.log('ü§ñ Simulando conversa com LLM...\n')
  
  // Intera√ß√£o 1
  console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')
  console.log('User: O que √© JSON‚úØAtomic?')
  const result1 = await llmWithSpan(
    'O que √© JSON‚úØAtomic?',
    { userId, sessionId, traceId }
  )
  console.log(`AI: ${result1.response}`)
  console.log()
  
  // Intera√ß√£o 2
  console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')
  console.log('User: Como funciona a assinatura Ed25519?')
  const result2 = await llmWithSpan(
    'Como funciona a assinatura Ed25519?',
    { userId, sessionId, traceId }
  )
  console.log(`AI: ${result2.response}`)
  console.log()
  
  // Intera√ß√£o 3
  console.log('‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ')
  console.log('User: Quais s√£o os casos de uso principais?')
  const result3 = await llmWithSpan(
    'Quais s√£o os casos de uso principais?',
    { userId, sessionId, traceId }
  )
  console.log(`AI: ${result3.response}`)
  console.log()
  
  console.log('‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê')
  console.log('‚úÖ Conversa completa rastreada!\n')
  
  // Verificar integridade de todos os spans
  console.log('üîç Verificando integridade dos spans...\n')
  
  const allSpans = [
    result1.spans.prompt,
    result1.spans.response,
    result2.spans.prompt,
    result2.spans.response,
    result3.spans.prompt,
    result3.spans.response
  ]
  
  for (const span of allSpans) {
    const valid = await verifySpan(span, { domain, publicKey })
    const status = valid ? '‚úÖ' : '‚ùå'
    console.log(`${status} ${span.type} (${span.id})`)
  }
  
  console.log()
  console.log('üìä Resumo da Sess√£o:')
  console.log(`   Session ID: ${sessionId}`)
  console.log(`   Trace ID: ${traceId}`)
  console.log(`   Total de intera√ß√µes: 3`)
  console.log(`   Total de spans: ${allSpans.length}`)
  console.log()
  
  console.log('üí° Benef√≠cios do rastreamento:')
  console.log('   ‚úÖ Auditoria completa de intera√ß√µes com AI')
  console.log('   ‚úÖ Debugging facilitado (trace IDs)')
  console.log('   ‚úÖ Compliance (GDPR, SOC2, etc.)')
  console.log('   ‚úÖ An√°lise de custos (tokens por usu√°rio)')
  console.log('   ‚úÖ Detec√ß√£o de abusos')
  console.log('   ‚úÖ Prova de que prompts n√£o foram alterados')
  console.log()
  
  console.log('üîó Use cases adicionais:')
  console.log('   - RAG (Retrieval-Augmented Generation) com spans')
  console.log('   - Chain-of-thought tracking')
  console.log('   - Multi-agent orchestration')
  console.log('   - Fine-tuning dataset generation')
  console.log('   - Cost optimization analytics')
}

main().catch(err => {
  console.error('‚ùå Erro:', err.message)
  process.exit(1)
})
